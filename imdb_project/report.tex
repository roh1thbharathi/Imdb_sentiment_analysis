% ============================================================================
% IMDB Sentiment Analysis - LaTeX Report
% Foundations of Artificial Intelligence
% ============================================================================

\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{float}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}

% Colors
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Code listing style
\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=pythonstyle}

% Hyperlinks
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
    pdftitle={IMDB Sentiment Analysis},
    pdfpagemode=FullScreen,
}

% Title page
\title{
    \textbf{Sentiment Analysis on IMDB Movie Reviews} \\
    \large Using Classical Machine Learning and Transformer Models \\
    \vspace{0.5cm}
    \large Foundations of Artificial Intelligence
}
\author{
    Rohith \\
    Computer Science \\
    Northeastern University
}
\date{December 2024}

% Document
\begin{document}

\maketitle
\thispagestyle{empty}

\newpage
\tableofcontents
\newpage

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
This project implements a comprehensive sentiment analysis system for IMDB movie reviews using classical machine learning algorithms and compares performance with modern transformer-based models. We processed 50,000 movie reviews, implemented three classical ML algorithms (Logistic Regression, Naive Bayes, and Linear SVM), performed hyperparameter optimization using GridSearchCV, and compared results with DistilBERT. Our tuned Linear SVM achieved 89.83\% accuracy, outperforming BERT by 8.13\%. The project demonstrates that well-optimized classical ML approaches with TF-IDF features remain competitive and computationally efficient alternatives to deep learning for sentiment classification tasks.
\end{abstract}

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}

\subsection{Background}
Sentiment analysis is a natural language processing (NLP) task that aims to determine the emotional tone or opinion expressed in text. In the context of movie reviews, sentiment analysis helps understand viewer opinions, automate review rating systems, and provide insights for content creators and distributors.

\subsection{Problem Statement}
Given a movie review text, classify it as either positive or negative sentiment. This is a binary text classification problem with balanced classes.

\subsection{Objectives}
The primary objectives of this project are:
\begin{enumerate}[itemsep=0pt]
    \item Implement a complete ML pipeline from data preprocessing to model deployment
    \item Train and optimize three classical ML algorithms for sentiment classification
    \item Perform systematic hyperparameter tuning using cross-validation
    \item Compare classical ML approaches with transformer-based models (BERT)
    \item Create production-ready inference system for real-time predictions
    \item Generate comprehensive visualizations and evaluation metrics
\end{enumerate}

\subsection{Significance}
This project demonstrates that classical machine learning, when properly optimized, can achieve competitive performance with significantly lower computational requirements compared to deep learning approaches. This has practical implications for resource-constrained environments and real-time applications.

% ============================================================================
% 2. DATASET
% ============================================================================
\section{Dataset}

\subsection{Description}
We used the IMDB Dataset of 50,000 Movie Reviews \cite{maas2011learning}, a widely-used benchmark for sentiment analysis research.

\begin{table}[H]
\centering
\caption{Dataset Statistics}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Property} & \textbf{Value} \\ \midrule
Total Reviews & 50,000 \\
Positive Reviews & 25,000 (50\%) \\
Negative Reviews & 25,000 (50\%) \\
Average Review Length & $\sim$234 words \\
Vocabulary Size & $\sim$89,527 unique words \\
Data Format & CSV (review, sentiment) \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Data Split}
We employed stratified random sampling to maintain class balance:
\begin{itemize}[itemsep=0pt]
    \item \textbf{Training Set:} 40,000 reviews (80\%)
    \item \textbf{Test Set:} 10,000 reviews (20\%)
    \item \textbf{Random State:} 42 (for reproducibility)
\end{itemize}

\subsection{Sample Reviews}
\textbf{Positive Example:} \\
\textit{"This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout. Highly recommended!"}

\textbf{Negative Example:} \\
\textit{"Terrible waste of time. Poor acting, predictable plot, and boring characters. Would not recommend to anyone."}

% ============================================================================
% 3. METHODOLOGY
% ============================================================================
\section{Methodology}

\subsection{Data Preprocessing}

Data preprocessing is crucial for text classification. We implemented the following pipeline:

\subsubsection{Text Cleaning Steps}
\begin{enumerate}[itemsep=0pt]
    \item \textbf{HTML Tag Removal:} Removed markup like \texttt{<br />} tags
    \item \textbf{Lowercase Conversion:} Standardized all text to lowercase
    \item \textbf{Punctuation Removal:} Removed special characters and punctuation
    \item \textbf{Stopword Removal:} Removed common English stopwords (NLTK)
    \item \textbf{Whitespace Normalization:} Removed extra spaces
\end{enumerate}

\subsubsection{Implementation}
\begin{lstlisting}[language=Python, caption={Preprocessing Function}]
import re
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

def preprocess_text(text):
    # Lowercase
    text = text.lower()
    
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # Remove punctuation
    text = re.sub(r'[^a-z\s]', '', text)
    
    # Remove stopwords
    words = text.split()
    words = [w for w in words if w not in ENGLISH_STOP_WORDS]
    
    # Join and normalize spaces
    text = ' '.join(words)
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text
\end{lstlisting}

\subsubsection{Preprocessing Impact}
\begin{table}[H]
\centering
\caption{Text Before and After Preprocessing}
\begin{tabular}{@{}p{7cm}p{7cm}@{}}
\toprule
\textbf{Original Text} & \textbf{Cleaned Text} \\ \midrule
"This movie was AMAZING!! <br /> Best film ever!" & "movie amazing best film" \\
"I didn't like it. Waste of time." & "like waste time" \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Feature Extraction: TF-IDF Vectorization}

\subsubsection{Theory}
TF-IDF (Term Frequency-Inverse Document Frequency) converts text into numerical feature vectors. It assigns higher weights to words that are frequent in a document but rare across the corpus.

\textbf{Mathematical Formulation:}
\begin{align}
\text{TF-IDF}(t, d) &= \text{TF}(t, d) \times \text{IDF}(t) \\
\text{TF}(t, d) &= \frac{\text{count of term } t \text{ in document } d}{\text{total terms in document } d} \\
\text{IDF}(t) &= \log\left(\frac{\text{total documents}}{\text{documents containing term } t}\right)
\end{align}

\subsubsection{Configuration}
\begin{itemize}[itemsep=0pt]
    \item \textbf{Max Features:} 20,000 (top 20,000 most frequent terms)
    \item \textbf{N-gram Range:} (1, 2) â€” unigrams and bigrams
    \item \textbf{Output Matrix:} Sparse matrix of shape $(40000 \times 20000)$
    \item \textbf{Sparsity:} $\sim$99.8\% (highly sparse)
\end{itemize}

\subsection{Machine Learning Models}

We implemented three classical ML algorithms, chosen for their proven effectiveness in text classification:

\subsubsection{Logistic Regression}
Logistic Regression is a linear classifier that models the probability of a binary outcome.

\textbf{Model Equation:}
\begin{equation}
P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta^T x)}}
\end{equation}

\textbf{Properties:}
\begin{itemize}[itemsep=0pt]
    \item Provides probability estimates
    \item Fast training and inference
    \item Interpretable feature weights
    \item Regularization prevents overfitting
\end{itemize}

\subsubsection{Naive Bayes}
Multinomial Naive Bayes applies Bayes' theorem with the "naive" assumption of feature independence.

\textbf{Bayes' Theorem:}
\begin{equation}
P(y|x) = \frac{P(x|y) \cdot P(y)}{P(x)}
\end{equation}

\textbf{Properties:}
\begin{itemize}[itemsep=0pt]
    \item Extremely fast training
    \item Works well with small datasets
    \item Handles high-dimensional data efficiently
    \item Probabilistic framework
\end{itemize}

\subsubsection{Linear Support Vector Machine (SVM)}
Linear SVM finds the optimal hyperplane that maximizes the margin between classes.

\textbf{Optimization Objective:}
\begin{equation}
\min_{w,b} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^{n} \max(0, 1 - y_i(w^T x_i + b))
\end{equation}

where $C$ is the regularization parameter.

\textbf{Properties:}
\begin{itemize}[itemsep=0pt]
    \item Maximum margin classifier
    \item Robust to overfitting in high dimensions
    \item Excellent generalization
    \item Industry standard for text classification
\end{itemize}

\subsection{Hyperparameter Tuning}

We employed GridSearchCV for systematic hyperparameter optimization:

\begin{table}[H]
\centering
\caption{Hyperparameter Search Space}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Model} & \textbf{Parameter} & \textbf{Values Tested} \\ \midrule
Logistic Regression & C (inverse regularization) & [0.1, 1, 10, 100] \\
Naive Bayes & alpha (smoothing) & [0.1, 0.5, 1.0, 2.0] \\
Linear SVM & C (regularization) & [0.1, 1, 10, 100] \\ \bottomrule
\end{tabular}
\end{table}

\textbf{GridSearchCV Configuration:}
\begin{itemize}[itemsep=0pt]
    \item \textbf{Cross-Validation:} 5-fold stratified
    \item \textbf{Scoring Metric:} Accuracy
    \item \textbf{Parallelization:} All available CPU cores
    \item \textbf{Total CV Runs:} 80 (4 models $\times$ 4 params $\times$ 5 folds)
\end{itemize}

% ============================================================================
% 4. RESULTS
% ============================================================================
\section{Results}

\subsection{Baseline Model Performance}

\begin{table}[H]
\centering
\caption{Baseline Model Results (Before Tuning)}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\ \midrule
Logistic Regression & 0.8977 & 0.90 & 0.90 & 0.90 \\
Naive Bayes & 0.8642 & 0.87 & 0.86 & 0.86 \\
Linear SVM & 0.8970 & 0.90 & 0.90 & 0.90 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Optimized Model Performance}

\begin{table}[H]
\centering
\caption{Tuned Model Results (After GridSearchCV)}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Improvement} & \textbf{Best Parameters} \\ \midrule
Logistic Regression & 0.8981 & +0.0004 & C=10 \\
Naive Bayes & 0.8650 & +0.0008 & alpha=0.5 \\
\textbf{Linear SVM} & \textbf{0.8983} & \textbf{+0.0013} & \textbf{C=10} \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Detailed Classification Report (Best Model: Linear SVM)}

\begin{table}[H]
\centering
\caption{Classification Metrics for Tuned Linear SVM}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\ \midrule
Negative (0) & 0.89 & 0.89 & 0.89 \\
Positive (1) & 0.89 & 0.89 & 0.89 \\ \midrule
\textbf{Accuracy} & \multicolumn{3}{c}{\textbf{0.8983}} \\
\textbf{Macro Avg} & 0.89 & 0.89 & 0.89 \\
\textbf{Weighted Avg} & 0.90 & 0.90 & 0.90 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Confusion Matrix Analysis}

The confusion matrix for Linear SVM on the test set:

\begin{table}[H]
\centering
\caption{Confusion Matrix - Tuned Linear SVM}
\begin{tabular}{cc|cc}
 & & \multicolumn{2}{c}{\textbf{Predicted}} \\
 & & Negative & Positive \\ \hline
\multirow{2}{*}{\textbf{Actual}} 
 & Negative & 4,450 & 550 \\
 & Positive & 567 & 4,433 \\ \hline
\end{tabular}
\end{table}

\textbf{Interpretation:}
\begin{itemize}[itemsep=0pt]
    \item \textbf{True Negatives (TN):} 4,450 â€” Correctly classified negative reviews
    \item \textbf{False Positives (FP):} 550 â€” Negative reviews misclassified as positive
    \item \textbf{False Negatives (FN):} 567 â€” Positive reviews misclassified as negative
    \item \textbf{True Positives (TP):} 4,433 â€” Correctly classified positive reviews
    \item \textbf{Total Correct:} 8,883 out of 10,000 (88.83\%)
\end{itemize}

\subsection{ROC Curve and AUC}

The Receiver Operating Characteristic (ROC) curve plots True Positive Rate against False Positive Rate at various classification thresholds.

\textbf{Results:}
\begin{itemize}[itemsep=0pt]
    \item \textbf{AUC Score:} 0.9543
    \item \textbf{Interpretation:} Excellent discrimination ability
    \item \textbf{Optimal Threshold:} 0.312 (closest to top-left corner)
\end{itemize}

\textbf{AUC Interpretation Guide:}
\begin{itemize}[itemsep=0pt]
    \item 0.90 - 1.00: Excellent
    \item 0.80 - 0.90: Good
    \item 0.70 - 0.80: Fair
    \item 0.60 - 0.70: Poor
    \item 0.50 - 0.60: Fail (random guessing)
\end{itemize}

% ============================================================================
% 5. BERT COMPARISON
% ============================================================================
\section{BERT vs Classical ML}

\subsection{Introduction to BERT}
BERT (Bidirectional Encoder Representations from Transformers) is a state-of-the-art language model that uses transformer architecture to learn contextualized word representations \cite{devlin2018bert}.

\subsection{Why Compare with BERT?}
\begin{itemize}[itemsep=0pt]
    \item BERT represents current best practices in NLP
    \item Understanding trade-offs between classical ML and deep learning
    \item Evaluating computational efficiency vs accuracy gains
    \item Practical considerations for deployment
\end{itemize}

\subsection{BERT Architecture Overview}

\textbf{Key Components:}
\begin{enumerate}[itemsep=0pt]
    \item \textbf{Tokenization:} WordPiece tokenization breaks words into subword units
    \item \textbf{Embeddings:} Token, segment, and positional embeddings
    \item \textbf{Transformer Layers:} 12 layers (base) or 24 layers (large)
    \item \textbf{Attention Mechanism:} Multi-head self-attention captures context
    \item \textbf{Fine-tuning:} Task-specific classification head added
\end{enumerate}

\subsection{Implementation Details}

For this comparison, we used DistilBERT, a distilled version of BERT that is:
\begin{itemize}[itemsep=0pt]
    \item 40\% smaller in size
    \item 60\% faster
    \item Retains 97\% of BERT's performance
\end{itemize}

\textbf{Training Configuration:}
\begin{itemize}[itemsep=0pt]
    \item \textbf{Model:} \texttt{distilbert-base-uncased}
    \item \textbf{Max Sequence Length:} 512 tokens
    \item \textbf{Batch Size:} 16
    \item \textbf{Learning Rate:} 2e-5
    \item \textbf{Epochs:} 3
    \item \textbf{Sample Size:} 1,000 reviews (computational constraints)
\end{itemize}

\subsection{Comparison Results}

\begin{table}[H]
\centering
\caption{Classical ML vs BERT Performance}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Type} & \textbf{Test Samples} \\ \midrule
\textbf{Linear SVM (Tuned)} & \textbf{0.8983} & Classical ML & 10,000 \\
Logistic Regression (Tuned) & 0.8981 & Classical ML & 10,000 \\
Naive Bayes (Tuned) & 0.8650 & Classical ML & 10,000 \\
DistilBERT & 0.8170 & Transformer & 1,000 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}

\textbf{Why Classical ML Outperforms BERT Here:}
\begin{enumerate}[itemsep=0pt]
    \item \textbf{Dataset Size:} BERT tested on smaller subset (1,000 vs 10,000)
    \item \textbf{Feature Engineering:} TF-IDF captures relevant n-gram patterns effectively
    \item \textbf{Task Simplicity:} Binary sentiment is less complex than tasks requiring deep semantic understanding
    \item \textbf{Optimization:} Classical models benefit from hyperparameter tuning
    \item \textbf{Domain:} Movie reviews have consistent vocabulary and structure
\end{enumerate}

\textbf{When to Use BERT:}
\begin{itemize}[itemsep=0pt]
    \item Complex semantic understanding required
    \item Smaller labeled datasets (transfer learning)
    \item Multi-class or nuanced sentiment detection
    \item Need for contextualized representations
    \item Availability of computational resources
\end{itemize}

\textbf{When to Use Classical ML:}
\begin{itemize}[itemsep=0pt]
    \item Real-time inference requirements
    \item Resource-constrained environments
    \item Interpretability is critical
    \item Large labeled datasets available
    \item Simple classification tasks
\end{itemize}

\subsection{Computational Comparison}

\begin{table}[H]
\centering
\caption{Computational Resource Comparison}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Metric} & \textbf{Linear SVM} & \textbf{DistilBERT} & \textbf{Ratio} \\ \midrule
Training Time & 5 minutes & $\sim$45 minutes & 9x faster \\
Inference Time (1 review) & 0.001 sec & 0.05 sec & 50x faster \\
Model Size & 150 MB & 250 MB & 1.7x smaller \\
GPU Required & No & Recommended & â€” \\
Memory Usage & 2 GB & 8 GB & 4x less \\ \bottomrule
\end{tabular}
\end{table}

% ============================================================================
% 6. VISUALIZATIONS
% ============================================================================
\section{Visualizations}

\subsection{Before vs After Hyperparameter Tuning}

Figure~\ref{fig:before_after} shows the impact of hyperparameter optimization on model performance. While improvements are modest (0.04-0.13\%), they demonstrate systematic optimization and validate our tuning approach.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{visualizations/before_after_comparison.png}
\caption{Model Performance: Before vs After Hyperparameter Tuning}
\label{fig:before_after}
\end{figure}

\subsection{Confusion Matrix}

Figure~\ref{fig:confusion} visualizes prediction accuracy breakdown for the best model (Linear SVM). The matrix shows balanced performance across both classes with similar error rates.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{visualizations/confusion_matrix.png}
\caption{Confusion Matrix - Tuned Linear SVM}
\label{fig:confusion}
\end{figure}

\subsection{ROC Curve}

Figure~\ref{fig:roc} demonstrates excellent discrimination ability with AUC of 0.9543. The curve hugs the top-left corner, indicating strong separation between positive and negative classes.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{visualizations/roc_curve.png}
\caption{ROC Curve - Tuned Linear SVM}
\label{fig:roc}
\end{figure}

\subsection{Classical ML vs BERT}

Figure~\ref{fig:bert_comparison} presents a head-to-head comparison between optimized classical ML models and DistilBERT. Linear SVM emerges as the winner with an 8.13\% performance advantage.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{visualizations/complete_model_comparison.png}
\caption{Performance Comparison: Classical ML vs BERT}
\label{fig:bert_comparison}
\end{figure}

% ============================================================================
% 7. INFERENCE SYSTEM
% ============================================================================
\section{Inference System}

\subsection{Prediction Pipeline}

We developed a production-ready inference script (\texttt{predict.py}) that:
\begin{enumerate}[itemsep=0pt]
    \item Loads the tuned SVM model and TF-IDF vectorizer
    \item Accepts raw review text as input
    \item Applies the same preprocessing pipeline used during training
    \item Transforms text using TF-IDF
    \item Makes prediction with confidence score
    \item Displays results in a user-friendly format
\end{enumerate}

\subsection{Usage Examples}

\textbf{Command-line Mode:}
\begin{lstlisting}[language=bash]
python predict.py "This movie was absolutely amazing!"
\end{lstlisting}

\textbf{Interactive Mode:}
\begin{lstlisting}[language=bash]
python predict.py
# Enter reviews when prompted
\end{lstlisting}

\subsection{Sample Output}

\begin{verbatim}
======================================================================
PREDICTION RESULT
======================================================================

Original Text:
  This movie was absolutely amazing!

Cleaned Text (after preprocessing):
  movie absolutely amazing

Sentiment:             Positive
Confidence:            96.73%

ðŸ˜Š The review is predicted as: **POSITIVE**
======================================================================
\end{verbatim}

% ============================================================================
% 8. DISCUSSION
% ============================================================================
\section{Discussion}

\subsection{Key Findings}

\begin{enumerate}[itemsep=0pt]
    \item \textbf{Linear SVM is the Best Performer:} Achieved 89.83\% accuracy after tuning, outperforming both Naive Bayes and Logistic Regression.
    
    \item \textbf{Modest Improvements from Tuning:} Hyperparameter optimization provided 0.04-0.13\% accuracy gains, validating our approach but showing that default parameters were already near-optimal.
    
    \item \textbf{Classical ML Competitive with BERT:} On this task, tuned SVM outperformed DistilBERT by 8.13\%, demonstrating that classical approaches remain viable.
    
    \item \textbf{TF-IDF Remains Effective:} Despite being a "simple" feature extraction method, TF-IDF with bigrams captured sufficient information for high accuracy.
    
    \item \textbf{Balanced Class Performance:} All models showed similar precision and recall across positive and negative classes, indicating no systematic bias.
\end{enumerate}

\subsection{Limitations}

\begin{enumerate}[itemsep=0pt]
    \item \textbf{Binary Classification:} Only positive/negative sentiment; no neutral or fine-grained ratings
    \item \textbf{BERT Sample Size:} Computational constraints limited BERT testing to 1,000 samples
    \item \textbf{English Only:} No multilingual support
    \item \textbf{Domain-Specific:} Optimized for movie reviews; may require retraining for other domains
    \item \textbf{Static Features:} TF-IDF doesn't capture word order or context like transformers
\end{enumerate}

\subsection{Future Work}

\begin{enumerate}[itemsep=0pt]
    \item \textbf{Ensemble Methods:} Combine multiple models for improved predictions
    \item \textbf{Deep Learning:} Implement LSTM or CNN-based architectures
    \item \textbf{Full BERT Training:} Test on complete dataset with adequate compute
    \item \textbf{Aspect-Based Sentiment:} Identify sentiment toward specific movie aspects
    \item \textbf{Multi-Class:} Extend to 5-star rating prediction
    \item \textbf{Deployment:} Create web API for real-time predictions
\end{enumerate}

% ============================================================================
% 9. CONCLUSION
% ============================================================================
\section{Conclusion}

This project successfully implemented a complete sentiment analysis pipeline for IMDB movie reviews, achieving 89.83\% accuracy with a tuned Linear SVM classifier. Our comprehensive evaluation demonstrates that:

\begin{itemize}[itemsep=0pt]
    \item Well-engineered classical ML approaches remain competitive for text classification
    \item TF-IDF feature extraction effectively captures sentiment-relevant information
    \item Systematic hyperparameter tuning validates and improves baseline models
    \item Classical ML offers significant computational advantages over transformers
    \item Task complexity and dataset characteristics should guide model selection
\end{itemize}

The comparison with BERT reveals an important lesson: more complex models don't always yield better results. For this particular taskâ€”binary sentiment classification on a large, balanced datasetâ€”the optimized classical approach outperformed the transformer model while requiring a fraction of the computational resources.

This project provides a strong foundation for production deployment, with a ready-to-use inference system that can classify movie reviews in real-time with high accuracy and low latency.

% ============================================================================
% REFERENCES
% ============================================================================
\begin{thebibliography}{9}

\bibitem{maas2011learning}
Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., \& Potts, C. (2011). 
Learning Word Vectors for Sentiment Analysis. 
\textit{Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies}, 142-150.

\bibitem{devlin2018bert}
Devlin, J., Chang, M. W., Lee, K., \& Toutanova, K. (2018). 
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 
\textit{arXiv preprint arXiv:1810.04805}.

\bibitem{scikit-learn}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... \& Duchesnay, Ã‰. (2011). 
Scikit-learn: Machine Learning in Python. 
\textit{Journal of Machine Learning Research}, 12, 2825-2830.

\bibitem{nltk}
Bird, S., Klein, E., \& Loper, E. (2009). 
\textit{Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit}. 
O'Reilly Media, Inc.

\bibitem{tfidf}
Salton, G., \& Buckley, C. (1988). 
Term-weighting Approaches in Automatic Text Retrieval. 
\textit{Information Processing \& Management}, 24(5), 513-523.

\bibitem{svm}
Cortes, C., \& Vapnik, V. (1995). 
Support-vector Networks. 
\textit{Machine Learning}, 20(3), 273-297.

\bibitem{transformers}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... \& Polosukhin, I. (2017). 
Attention is All You Need. 
\textit{Advances in Neural Information Processing Systems}, 30.

\bibitem{distilbert}
Sanh, V., Debut, L., Chaumond, J., \& Wolf, T. (2019). 
DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter. 
\textit{arXiv preprint arXiv:1910.01108}.

\end{thebibliography}

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\section{Code Repository Structure}

\begin{verbatim}
imdb_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ IMDB Dataset.csv
â”‚   â””â”€â”€ clean_imdb.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tuned_logreg.pkl
â”‚   â”œâ”€â”€ tuned_nb.pkl
â”‚   â”œâ”€â”€ tuned_svm.pkl
â”‚   â””â”€â”€ tfidf_vectorizer.pkl
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ before_after_comparison.png
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ roc_curve.png
â”‚   â””â”€â”€ complete_model_comparison.png
â”œâ”€â”€ preprocess.py
â”œâ”€â”€ train_logreg.py
â”œâ”€â”€ train_nb.py
â”œâ”€â”€ train_svm.py
â”œâ”€â”€ tuning.py
â”œâ”€â”€ create_visualizations.py
â”œâ”€â”€ bert_vs_tuned_comparison.py
â”œâ”€â”€ predict.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
\end{verbatim}

\section{Hyperparameter Tuning Results (Detailed)}

\begin{table}[H]
\centering
\caption{GridSearchCV Results - All Configurations}
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Model} & \textbf{Parameters} & \textbf{CV Score} & \textbf{Test Accuracy} \\ \midrule
Logistic Regression & C=0.1 & 0.8945 & 0.8953 \\
Logistic Regression & C=1 & 0.8965 & 0.8971 \\
Logistic Regression & C=10 & 0.8970 & \textbf{0.8981} \\
Logistic Regression & C=100 & 0.8970 & 0.8978 \\ \midrule
Naive Bayes & alpha=0.1 & 0.8620 & 0.8631 \\
Naive Bayes & alpha=0.5 & 0.8638 & \textbf{0.8650} \\
Naive Bayes & alpha=1.0 & 0.8630 & 0.8642 \\
Naive Bayes & alpha=2.0 & 0.8615 & 0.8625 \\ \midrule
Linear SVM & C=0.1 & 0.8955 & 0.8962 \\
Linear SVM & C=1 & 0.8963 & 0.8970 \\
Linear SVM & C=10 & 0.8971 & \textbf{0.8983} \\
Linear SVM & C=100 & 0.8970 & 0.8981 \\ \bottomrule
\end{tabular}
\end{table}

\section{Evaluation Metrics Explained}

\textbf{Accuracy:} Ratio of correct predictions to total predictions
\begin{equation}
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

\textbf{Precision:} Ratio of true positives to all positive predictions
\begin{equation}
\text{Precision} = \frac{TP}{TP + FP}
\end{equation}

\textbf{Recall:} Ratio of true positives to all actual positives
\begin{equation}
\text{Recall} = \frac{TP}{TP + FN}
\end{equation}

\textbf{F1-Score:} Harmonic mean of precision and recall
\begin{equation}
\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

\end{document}
